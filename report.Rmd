---
title: "Weather in the United States (1996-2023)"
author: "by Just Commit (Group number: 17): Olivia Harris, Maxwell Pohlmann, Sarah Stewart, Helen Miller, Andrew Morris, & Adam Laycock"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-lib}
library(tidyverse)
library(tidymodels)
library(janitor) #converts variable names to snake_case
library(tidyverse)
library(janitor)
library(workflows)
library(parsnip)
library(tidyclust)
library(tidymodels)
library(usmap)
library(gghighlight)
library(knitr)
library(xaringanExtra)
library(doParallel)
xaringanExtra::use_panelset()
library(tidyverse)
library(tidymodels)
library(janitor)
library(broom)
library(patchwork)
```

```{r load-data}
details = read_csv('data/original_data/details_combined.csv')
fatalities = read_csv('data/original_data/fatalities_combined.csv')
```

```{r Data Cleaning}
# Merge and convert formats of begin-date variables
details <- details %>%
  # Combine date elements
  unite("BEGIN_DATE", BEGIN_YEARMONTH, BEGIN_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    BEGIN_DATE = as.character(BEGIN_DATE),
    # Add leading zero if the time is 3 digits
    BEGIN_TIME = case_when(
      BEGIN_TIME < 1000 ~ sprintf("%04d", BEGIN_TIME),
      TRUE ~ as.character(BEGIN_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("BEGIN_DT", BEGIN_DATE, BEGIN_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    BEGIN_DT = ymd_hm(BEGIN_DT)
  )

# Merge and convert formats of end-date variables
details <- details %>%
  # Combine date elements
  unite("END_DATE", END_YEARMONTH, END_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    END_DATE = as.character(END_DATE),
    # Add leading zero if the time is 3 digits
    END_TIME = case_when(
      END_TIME < 1000 ~ sprintf("%04d", END_TIME),
      TRUE ~ as.character(END_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("END_DT", END_DATE, END_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    END_DT = ymd_hm(END_DT)
  )

# Remove unnecessary columns
details <- details %>%
  select(
    -c(YEAR, MONTH_NAME, BEGIN_DATE_TIME, END_DATE_TIME, 
       EVENT_NARRATIVE, EPISODE_NARRATIVE
    )
  )

# Rename incorrect column names
details <- details %>%
  rename(
    REGION = STATE,
    REGION_FIPS = STATE_FIPS
  )

# Create duration variable 
details <- details %>% 
  mutate(
    duration = END_DT - BEGIN_DT
  ) %>% 
    mutate(
      duration = as.numeric(duration) / 60
    )

# Change cost suffixes
details <- details %>% 
  mutate(
    DAMAGE_PROPERTY = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_PROPERTY))),
    DAMAGE_CROPS = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_CROPS)))
  )

# Create a total damage column using crops and property
details <- details %>%
  mutate(
    DAMAGE_TOTAL = DAMAGE_PROPERTY + DAMAGE_CROPS
  )

# Remove all records pertaining to an incomplete year
details <- details %>% 
  filter(
    !year(BEGIN_DT) == '2024'
  )

# Use janitor to convert variable names to snakecase
details <- details %>% 
  clean_names()

# Tidy the fatalities DataFrame
fatalities <- fatalities %>% 
  select(
    -FAT_TIME, -FATALITY_DATE, -EVENT_YEARMONTH
  ) %>% 
    mutate(
      YMD = ymd(paste(FAT_YEARMONTH, FAT_DAY))
    ) %>% 
      select(
        -FAT_DAY, -FAT_YEARMONTH
      )

# Remove 2024 records
fatalities <- fatalities %>%
  filter(
    !year(YMD) == '2024'
  )

# Use janitor to convert variables to snakecase
fatalities <- fatalities %>% 
  clean_names()

details <- details %>%
  mutate(
    event_type = recode(
      event_type,
      "Hurricane (Typhoon)" = "Hurricane"
    )
  )

#write_rds(details, "data/clean_data/details_clean.rds", compress = "gz")
#write_rds(fatalities, "data/clean_data/fatalities_clean.rds")
```

## Research Question

How have storm events changed over time, and what are the personal and monetary effects of these changes?

We chose to investigate this question due to the increasingly dramatic shifts in climate observed in recent years.
Many of these changes seem to be significantly impacting the United States, and we wanted to explore those impacts in our data.

## Data

Our data was sourced from the Storm Events Database of the National Oceanic and Atmospheric Administration (NOAA).
The NOAA collects data on weather events in the United States.
This includes information on event location, 50 variables related to event details (including, notably, time of occurance and damages), and 10 variables related to event fatalities.
In our analysis, we focused on over 1.6 million observations collected between 1996 and 2023.

In our initial cleaning, we notably removed unwanted variables such as event narrative (a multi-sentence description of the event), merged date columns and converted them to date-times with `lubridate,` and added a column for total damage cost (summing crop and property damage).
All damage information was converted from a string to a double.
The cleaning script can be found in it's entirety in r `cleaning_script`.

```{r cleaning_script}
# Merge and convert formats of begin-date variables
details <- details %>%
  # Combine date elements
  unite("BEGIN_DATE", BEGIN_YEARMONTH, BEGIN_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    BEGIN_DATE = as.character(BEGIN_DATE),
    # Add leading zero if the time is 3 digits
    BEGIN_TIME = case_when(
      BEGIN_TIME < 1000 ~ sprintf("%04d", BEGIN_TIME),
      TRUE ~ as.character(BEGIN_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("BEGIN_DT", BEGIN_DATE, BEGIN_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    BEGIN_DT = ymd_hm(BEGIN_DT)
  )

# Merge and convert formats of end-date variables
details <- details %>%
  # Combine date elements
  unite("END_DATE", END_YEARMONTH, END_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    END_DATE = as.character(END_DATE),
    # Add leading zero if the time is 3 digits
    END_TIME = case_when(
      END_TIME < 1000 ~ sprintf("%04d", END_TIME),
      TRUE ~ as.character(END_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("END_DT", END_DATE, END_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    END_DT = ymd_hm(END_DT)
  )

# Remove unnecessary columns
details <- details %>%
  select(
    -c(YEAR, MONTH_NAME, BEGIN_DATE_TIME, END_DATE_TIME, 
       EVENT_NARRATIVE, EPISODE_NARRATIVE
    )
  )

# Rename incorrect column names
details <- details %>%
  rename(
    REGION = STATE,
    REGION_FIPS = STATE_FIPS
  )

# Create duration variable 
details <- details %>% 
  mutate(
    duration = END_DT - BEGIN_DT
  ) %>% 
    mutate(
      duration = as.numeric(duration) / 60
    )

# Change cost suffixes
details <- details %>% 
  mutate(
    DAMAGE_PROPERTY = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_PROPERTY))),
    DAMAGE_CROPS = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_CROPS)))
  )

# Create a total damage column using crops and property
details <- details %>%
  mutate(
    DAMAGE_TOTAL = DAMAGE_PROPERTY + DAMAGE_CROPS
  )

# Remove all records pertaining to an incomplete year
details <- details %>% 
  filter(
    !year(BEGIN_DT) == '2024'
  )

# Use janitor to convert variable names to snakecase
details <- details %>% 
  clean_names()

# Tidy the fatalities DataFrame
fatalities <- fatalities %>% 
  select(
    -FAT_TIME, -FATALITY_DATE, -EVENT_YEARMONTH
  ) %>% 
    mutate(
      YMD = ymd(paste(FAT_YEARMONTH, FAT_DAY))
    ) %>% 
      select(
        -FAT_DAY, -FAT_YEARMONTH
      )

# Remove 2024 records
fatalities <- fatalities %>%
  filter(
    !year(YMD) == '2024'
  )

# Use janitor to convert variables to snakecase
fatalities <- fatalities %>% 
  clean_names()

details <- details %>%
  mutate(
    event_type = recode(
      event_type,
      "Hurricane (Typhoon)" = "Hurricane"
    )
  )

#save cleaned data
#write_rds(details, "../clean_data/details_clean.rds", compress = "gz")
#write_rds(fatalities, "../clean_data/fatalities_clean.rds")
```





```{r load-data}
details = read_rds('../../data/clean_data/details_clean.rds')
fatalities = read_rds('../../data/clean_data/fatalities_clean.rds')
```



## Changes in Storm Event Frequency Over Time

To identify how storm events have changed over time we used a linear regression model attempting to predict the number of storm events per year.
We grouped event frequencies by year to balance statistical significance and trend visibility as smaller groups were skewed by outliers, while larger ones (e.g., 5-year periods) lacked sufficient detail to show trends over time.
To do this, we created the variable events_per_year by summing event frequencies annually and ran the regression: lm(event_count ~ year, data = events_per_year).
The results predicted for every one-year increase there will be, on average, an additional 774.2 storm events per year.
These results are statistically significant (p > 0.01, R-squared = 0.5358) with over half of the variation in event count per year explained by the variable year.

We created similar models predicting fatalities and damages per year.
These models predict that for every additional year, on average, fatalities will increase by 15.3 and damages will increase by $447 million.
Both these results have p-values < 0.05 and are statistically significant.
However, the R-squared values are low (0.3 for damages and 0.2 for fatalities), indicating there are other factors with a significant influence.







```{r filter events}
severe_details <- details %>%
  select(-c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, tor_length, tor_width, tor_other_wfo, tor_other_cz_state, tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, begin_range, begin_location, end_range)) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) %>%
  distinct(event_type, .keep_all = TRUE) 

```

```{r not severe}
not_severe_details <- details %>%
  select(-c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, tor_length, tor_width, tor_other_wfo, tor_other_cz_state, tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, begin_range, begin_location, end_range)) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) 
```

```{r joining}
combined_data <- severe_details %>%
  full_join(fatalities, by = "event_id")
```
 
```{r not combined}
not_combined_data <- not_severe_details %>%
  full_join(fatalities, by = "event_id")
```

When analyzing the total number of weather events, we found Texas experienced significantly more weather events than any other state. This could potentially be explained by its considerably large size. 

```{r not severe frequency events}
not_frequency_by_region <-not_combined_data%>%
  group_by(region) %>%
  summarize(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
not_total_events <- not_combined_data %>%
  summarize(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(not_total_events= total_events)
 
not_combined_data <- not_combined_data %>%
  left_join(not_frequency_by_region, by="region")
 
```
 
```{r not severe event frequency map}
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(not_frequency_by_region, by = "region") 
 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of events") + 
  labs(
    title = "Number of all weather events experienced per state") +
  theme_minimal()
```

With the filtered data, analyzed which states experienced severe weather events and how often. From this we found Florida, Texas and California endured the highest frequency of severe weather events. 

```{r  frequency events}
frequency_by_region <-combined_data%>%
  group_by(region) %>%
  summarize(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
total_events <- combined_data %>%
  summarize(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
frequency_by_region <- frequency_by_region %>%
  mutate(total_events= total_events)
 
combined_data <- combined_data %>%
  left_join(frequency_by_region, by="region")
 
```
 
```{r event frequency map}
 
combined_data <- combined_data %>% 
  mutate(region = tolower(region))
 
frequency_by_region <- frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(frequency_by_region, by = "region") 
 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of severe events") + 
  labs(
    title = "Map of the USA",
    subtitle = "Most severe weather events experienced per state") +
  theme_minimal()
```

When comparing this data to the total number of fatalities caused by all weather events, we found that, as predicted, Florida, California, and Texas all experienced a high fatality rate. However, unexpectedly Arizona showed a high number of fatalities, even though they did not experience a lot of severe weather. 

```{r not fatalities  map}
 
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
fatalities_by_region <- fatalities_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(fatalities_by_region, by = "region") 
 
map_combined
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_fatalities)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of fatalities") + 
  labs(
    title = "Number of fatalities per state") +
  theme_minimal()
```

Finally, we compared the results found to the total damages caused by severe weather events and much of what we discovered was to be expected, with Florida, Texas, California and Arizona all having experienced a high degree of total damages caused by severe weather events. However, many states experienced similar high damage costs while experiencing drastically less frequent severe weather events. From this we concluded that it could potentially stem from the lack of sufficient infrastructure in place to prepare for storm events, especially compared to states experiencing severe weather events frequently. 

```{r map damage}
 
#load map data
combined_data <- combined_data %>% 
      mutate(region = tolower(region))
             
usa_map <- map_data("state")     
             
map_combined<- combined_data%>% 
  full_join(usa_map)
 
# Plot the map
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = log(sum_damage_total)
    )
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Total Damage ($)") + 
  labs(title = "States experiencing the most damage") +
  theme_minimal()
```










# Damage Makeup by Event Type

```{r fig.align='center'}
cost_data <- details %>% 
  select(
    event_type, damage_property, damage_crops
  ) %>% 
    group_by(
      event_type
    ) %>% 
      summarise(
        Property = sum(
          damage_property, 
          na.rm=TRUE
        ),
        Crops = sum(
          damage_crops, 
          na.rm=TRUE
        )
      ) %>%
        slice_max(
          order_by = Property + Crops, 
          n = 20
        ) %>% 
          pivot_longer(
            cols=c('Property', 'Crops'),
            names_to='type',
            values_to='damage'
          )

cost_data %>% 
  ggplot(
    mapping=aes(
      x=damage/sum(damage)*100,
      y=reorder(event_type, damage),
      fill=type
    )
  ) +
  geom_col() +
  labs(
    title = 'Damage by Storm Event Type between 1996 & 2023', 
    subtitle = 'Total Damage (Property + Crops) was $227bn',
    y = 'Type of Event',
    x = 'Pecentage of Total Damage',
    fill='Damage Type',
    caption='Only the 20 most damaging event types are shown.'
  ) +
  theme_minimal() +
  theme(
    plot.caption = element_text(hjust = -0.75, face= "italic"),
    aspect.ratio=4/4
  )
```
This plot shows that not all highly damaging events caused the same types of damages. Flash floods and tornadoes predominantly caused damage to property, while droughts and freezes disproportionately affected crops.

# Event Severity Clustering Using K-means
## Preprocessing & Optimisation
### Feature Selection & Engineering
```{r Feature Selection & Engineering}
# Create combined metrics by combining direct and indirect
details <- details %>% 
  mutate(
    attributed_deaths = deaths_direct + deaths_indirect,
    attributed_injuries = injuries_direct + injuries_indirect,
    attributed_casualties = attributed_deaths + attributed_injuries
  )

# Select only relevant data and remove rows with NA
model_data <- details %>% 
  select(
    event_id, attributed_casualties, damage_total, event_type
  ) %>% 
    filter(
      !is.na(damage_total) & !is.na(attributed_casualties)
    )

# Z-Score normalise the data
model_data_scaled <- model_data %>%
  mutate(
    damage_total = as.vector(scale(damage_total)),
    attributed_casualties = as.vector(scale(attributed_casualties))
)
```

### K Hyperparameter Optimisation
```{r Cluster number optimisation, fig.align='center', eval=FALSE}
# Initialise empty lists
k_list = list()
sse_list = list()

# Loop through 1-30 clusters and store total sum of squared error
for (k in 1:30) {
  kmeans_spec <- k_means(num_clusters = k)
  
  kmeans_fit <- kmeans_spec %>%
    fit(~ damage_total + attributed_casualties, data = model_data_scaled)
  
  kmeans_result <- kmeans_fit$fit
  sse <- kmeans_result$tot.withinss
  
  k_list <- c(k_list, k)
  sse_list <- c(sse_list, sse)
}

# Convert lists to a DataFrame
elbow_data <- data.frame(unlist(k_list), unlist(sse_list))
names(elbow_data) = c("k","total_sse")


# Create elbow plot using clustering data
elbow_data %>% 
  ggplot(
    mapping=aes(
      x=k,
      y=total_sse
    )
  ) + 
  geom_line() +
  geom_point() + 
  scale_x_continuous(breaks = seq(1, 30, by = 1)) +
  labs(
    title='Total SSE by Number of Clusters', 
    x='Number of Clusters (k)',
    y='Total SSE'
  ) +
  theme_minimal()
```

## Fitting the Optimised Model
```{r Optimised Model}
# Create a model object using the optimum number of clusters
kmeans_spec <- k_means(num_clusters = 3)

# Fit the model
kmeans_fit <- kmeans_spec %>%
  fit(~ damage_total + attributed_casualties, data = model_data_scaled)

# Add clusters to data
model_data_scaled <- kmeans_fit %>% 
  augment(
    model_data_scaled
  )

# Change cluster names
model_data_scaled <- model_data_scaled %>% 
  rename(
    cluster = .pred_cluster
  ) %>% 
  mutate(
    cluster = case_when(
      cluster == 'Cluster_1' ~ 'Low Damage / Low Casualties',
      cluster == 'Cluster_2' ~ 'High Damage / Low Casualties',
      TRUE ~ 'Low Damage / High Casualties'
    )
  )
```

## Visualising Clustering Results
### Scatterplot using Clusters
```{r Scatter Plot of Scaled Data by Cluster, fig.align='center'}
# Create a scatterplot of the features, hued by cluster
model_data_scaled %>%
  ungroup() %>% 
    ggplot(
      mapping=aes(
        x=damage_total,
        y=attributed_casualties,
        colour=cluster
      )
    ) +
    geom_jitter() +
    labs(
      x='Scaled Total Damage',
      y='Scaled Attributed Casualties',
      title='K-Means Clustering of Casualties & Damages',
      colour='Cluster'
    ) +
    theme_minimal()
```

### Tabular Clusters
```{r Cluster Frequency Table, fig.align='center'}
# Create a table for the number of events in each cluster
model_table <- model_data_scaled %>% 
  group_by(
    cluster
  ) %>% 
    summarise(
      total = n()
    ) %>%
      rename(
        Cluster = cluster,
        Total = total
      ) %>% 
        arrange(
          desc(Total)
        )

kable(model_table)
```

Across the assessed time period, the vast majority of storm events fell into the low severity category with only a relative handful being found in the extremes of high damage or high casualties.

# Geospatial Correlation Between Events & Time
## Building the Function
```{r Map Correlations Between Number of Events & Time}
# Define function
map_correlations <- function(event_type) {
  # Defuse argument for dplyr
  event_type_expr <- enquo(event_type)
  
  # Group by region and year
  total_region_data <- details %>%
    select(
      begin_dt, event_type, region
    ) %>% 
      group_by(
        region, year(begin_dt)
      ) %>%
        summarise(
          ov_total = n()
        ) %>% 
          rename(
            year = `year(begin_dt)`
          )

  # Evaluate and filter by event type, join to total events
  region_data <- details %>%
    filter(
      event_type == !!event_type_expr
    ) %>% 
      select(
        begin_dt, event_type, region
      ) %>% 
        group_by(
          region, year(begin_dt)
        ) %>%
          summarise(
            total = n()
          ) %>% 
            arrange(
              desc(total)
            ) %>% 
              rename(
                year = `year(begin_dt)`
              ) %>% 
                inner_join(
                  total_region_data, by=c('region', 'year')
                ) %>% 
                  mutate(
                    prop = total / ov_total
                  )

  # Run pearson's rank between time & event proportion
  correlation_data <- region_data %>%
    group_by(
      region
    ) %>%
      summarise(
        # Continue through common, non-fatal errors
        corr_test = list(tryCatch(
          cor.test(
            year, 
            prop, 
            method = "pearson", 
            use = "complete.obs"
          ), 
          error = function(e) NULL
        ))
      ) %>%
        # Add correlation and p-values into variables
        mutate(
          corr = sapply(corr_test, function(test) if (!is.null(test)) test$estimate else NA),
          p_value = sapply(corr_test, function(test) if (!is.null(test)) test$p.value else NA)
        ) %>%
          # Remove NA values and statistically insignificant results
          filter(
            !is.na(corr) & !is.na(p_value) & p_value <= 0.05
          ) %>%
            select(
              region, corr, p_value
            ) %>%
              mutate(
                event_type = event_type
              )
  
  # Build a map of the US
  map_data <- usmap::us_map(regions = "states")
  
  # Mutate map_data to have parity with correlation data
  map_data <- map_data %>% 
    mutate(
      full = tolower(full)
    ) %>% 
      rename(
        region = full
      )
  
  # Alter correlation region data
  correlation_data <- correlation_data %>% 
    mutate(
      region = tolower(region)
    )
  
  # Join correlation and map data
  data <- left_join(map_data, correlation_data, by='region')
  
  # Build the map showing correlations
  plot_usmap(
    data=data, 
    values='corr'
  ) +
  scale_fill_continuous(name = "Correlation \nCoefficient") +
  theme(legend.position = "right") +
  labs(
    title=paste('Correlations Between Year &', event_type, 'Events as a Proportion of Total State Events'
          ),
    subtitle = 'Only statistically significant (p<=0.05) correlation coefficients are shown'
  )
}
```

## Using the Function {.panelset}

### Thunderstorm Wind

#### Thunderstorm Wind Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Thunderstorm Wind')
```

### Droughts

#### Drought Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Drought')
```

### Hurricanes

#### Hurricane Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Hurricane')
```

##
These plots highlight a key limitation with this script, rarer events are much harder to track and predict through time. As a result, lots of the potential correlations for rarer events, such as hurricanes or droughts, are not shown due to the statistical insignificance owing to a smaller sample size.

# Deadly Storm Predictor
## Preprocessing
### Response & Sampling
```{r}
# Create new deady variable based on deaths
details <- details %>% 
  mutate(
    deadly = case_when(
      deaths_direct > 0 | deaths_indirect > 0 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    )
  )

# Select only relevant variables
model_data <- details %>% 
  select(
    begin_dt, region, event_type, deadly
  )

# Take a large sample of the data
model_data <- model_data %>% 
  sample_n(
    1000000
  )
```

### Train, Test Split
```{r}
# Set seed and split data
set.seed(1)
storm_split <- initial_split(model_data)
storm_train <- training(storm_split)
storm_test  <- testing(storm_split)
```

### Building the Recipe, Model, & Workflow
```{r}
# Build preprocessing recipe
storm_rec_1 <- recipe(
  deadly ~ .,
  data = storm_train
) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# Declare model
storm_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")
  
# Build workflow using recipe and model
storm_wflow_1 <- workflow() %>%
  add_recipe(storm_rec_1) %>%
  add_model(storm_mod_1)
```

## Training the Model
### Fitting the Model Using Parallel Processing
```{r message=FALSE, warning=FALSE}
# Use parallel processing to speed up fit
registerDoParallel(cores = detectCores() - 1)

# Fit and store the model
storm_fit_1 <- fit(storm_wflow_1, data = storm_train)
```

### Extracting Prediction Probabilities
```{r message=FALSE, warning=FALSE}
# Create prediction probabilities and bind to test data
storm_pred <- predict(
  storm_fit_1, 
  storm_test, 
  type = "prob"
) %>%
  bind_cols(
    storm_test
  )
```

## Analysing Model Fit
### ROC Curve & AUC Value
```{r fig.align='center'}
# Convert response variable to a factor
storm_pred <- storm_pred %>% 
  mutate(
    deadly = as.factor(deadly)
  )

# Calculate an AUC score
auc_score <- storm_pred %>% 
  roc_auc(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  )

# Build ROC curve and attach AUC score
storm_pred %>%
  roc_curve(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  ) %>%
  autoplot() +
  labs(
    title = 'ROC Curve for Deadly Storm Predictor',
    x = 'False Positive Rate / 1 - Specificity',
    y = 'True Positive Rate / Sensitivity'
  ) +
  geom_text(
    x = 0.6,
    y = 0.45,
    label = paste("AUC = ", round(auc_score$.estimate, 2))
  )
```

### Building a Confusion Matrix
```{r message=FALSE, warning=FALSE, fig.align='center'}
# Use probability cutoff to calculate model prediction
storm_pred <- storm_pred %>% 
  mutate(
    prediction = case_when(
      .pred_Deadly >= 0.5 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    ),
    prediction = as.factor(prediction)
  )

# Construct and a confusion matrix object and convert it to a tibble
conf_mat <- conf_mat(
  data = storm_pred,
  truth = deadly,
  estimate = prediction
) %>%
  tidy()

# Extract values from the tibble to build a new, clean tibble
confusion <- tibble(
  truth = c('Deadly', 'Not Deadly', 'Deadly', 'Not Deadly'),
  prediction = c('Deadly', 'Not Deadly', 'Not Deadly', 'Deadly'),
  n = c(conf_mat[1,2], conf_mat[4,2], conf_mat[2,2], conf_mat[3,2])
)

# Plot a confusion matrix
confusion %>% 
  ggplot(
    mapping = aes(
      x = truth, 
      y = prediction
    )
  ) +
  geom_tile(fill = "steelblue",
            color = "black",
            size = 0.5
  ) +
  geom_text(
    mapping = aes(
      label = n
    ), 
    color = "black", 
    size = 5
  ) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix for Deadly Storm Predictor",
    x = "Model Prediction", 
    y = "Truth"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = 'none'
  )
```

### Accuracy
```{r}
# Calculate an accuracy metric
storm_pred %>% 
  mutate(
    result = case_when(
      deadly == prediction ~ 'Correct',
      TRUE ~ 'Incorrect'
    )
  ) %>% 
    group_by(
      result
    ) %>% 
      summarise(
        percentage = n()/250000*100
      ) %>%
        rename(
        Result = result,
        Percentage = percentage
      ) %>% 
        kable()
```









 
 
# Event Type Averages and Data Limitations 
 
We split event types into two categories: more/less damaging/deadly (compared to the mean). This was used to assess whether increased damages and fatalities over time were reflected equally in both severe and less severe storm events. 
 
```{r chunk_1}
# Rename redundant event type names
  details <- details %>%
    mutate(event_type = replace(event_type, event_type == "Hurricane (Typhoon)", "Hurricane"))
 
# Select relevant columns and replace NA values
details_selected <- details %>%
  select(event_type, deaths_direct, deaths_indirect, damage_total, begin_dt) %>%  
  mutate(
    year = as.numeric(format(as.Date(begin_dt, format = "%Y-%m-%d"), "%Y"))  
  ) %>%
  na.omit()
 
# Calculate total fatalities and damage per event, and group by event type
details_grouped <- details_selected %>%
  mutate(
    fatalities_total = deaths_direct + deaths_indirect
  ) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),  
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)  
  ) %>%
  ungroup()
 
# Calculate global averages and assign groups
global_averages <- details_grouped %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),  
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)  
  )
 
# Add grouping for more/less deadly and damaging based on global averages
details_grouped <- details_grouped %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
```
We used mean impact for the analysis of changes in fatalities and damages due to the possibility of stronger storms overall or greater storm frequency over time. 
 
```{r chunk_2}
# Adjusted Graph for Fatalities (All Data)
fatality_graph <- details_grouped %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("More Deadly" = "red", "Less Deadly" = "green")) +
  labs(
    title = "Average Fatalities by Group",
    subtitle = "Based on Event Type Averages Compared to Global Average",
    x = "Weather Group",  
    y = "Average Fatalities per Event Type",
    fill = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.subtitle = element_text(size = 6.5),  
    axis.text.x = element_text(size = 5)  
  )
 
# Adjusted Graph for Damages (All Data)
damage_graph <- details_grouped %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("More Damaging" = "purple", "Less Damaging" = "orange")) +
  labs(
    title = "Average Damages by Group",
    subtitle = "Based on Event Type Averages Compared to Global Average",
    x = "Weather Group", 
    y = "Average Damages per Event Type (USD)",
    fill = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.subtitle = element_text(size = 6.5), 
    axis.text.x = element_text(size = 5)  
  )
 
# Combine  plots
combined_original <- fatality_graph + damage_graph
 
# Display plots
combined_original
```
 
We repeated this process for 1996–7 and 2022–3, using two-year periods to minimise the impact of outliers. 
```{r chunk_3 / table_1}
# Filter for 1996
details_1996_7 <- details_selected %>% filter(year %in% c(1996, 1997))
details_grouped_1996_7 <- details_1996_7 %>%
  mutate(fatalities_total = deaths_direct + deaths_indirect) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)
  ) %>%
  ungroup()
 
# Recalculate global averages for 1996 data
global_averages_1996_7 <- details_grouped_1996_7 %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)
  )
 
# Add grouping for 1996 data
details_grouped_1996_7 <- details_grouped_1996_7 %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages_1996_7$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages_1996_7$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
```
 
### Labeled graph for the 1996-7 period:
```{r chunk_4, echo=FALSE}
# Adjusted Graph for 1996 Fatalities
fatality_graph_1996_7 <- details_grouped_1996_7 %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "1996-7: Fatalities by Group", subtitle = "Data from 1996-7", 
       x = "Weather Group",  
       y = "Average Fatalities") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5) 
  )
 
# Adjusted Graph for 1996 Damages
damage_graph_1996_7 <- details_grouped_1996_7 %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "1996-7: Damages by Group", subtitle = "Data from 1996-7", 
       x = "Weather Group",  
       y = "Average Damages (USD)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Combine plots
combined_1996_7 <- fatality_graph_1996_7 + damage_graph_1996_7
 
# Display plots
combined_1996_7
```
 
### Labeled graph for the 2022-3 period:
```{r chunk_5, echo=FALSE}
# Filter for 2022-3
details_2022_3 <- details_selected %>% filter(year %in% c(2022, 2023))
details_grouped_2022_3 <- details_2022_3 %>%
  mutate(fatalities_total = deaths_direct + deaths_indirect) %>%
  group_by(event_type) %>%
  summarise(
    avg_fatalities_by_type = mean(fatalities_total, na.rm = TRUE),
    avg_damage_by_type = mean(damage_total, na.rm = TRUE)
  ) %>%
  ungroup()
 
# Recalculate global averages for 2022-3 data
global_averages_2022_3 <- details_grouped_2022_3 %>%
  summarise(
    global_avg_fatalities = mean(avg_fatalities_by_type, na.rm = TRUE),
    global_avg_damage = mean(avg_damage_by_type, na.rm = TRUE)
  )
 
# Add grouping for 2022-3 data
details_grouped_2022_3 <- details_grouped_2022_3 %>%
  mutate(
    fatality_group = if_else(avg_fatalities_by_type > global_averages_2022_3$global_avg_fatalities,
                             "More Deadly", "Less Deadly"),
    damage_group = if_else(avg_damage_by_type > global_averages_2022_3$global_avg_damage,
                           "More Damaging", "Less Damaging")
  )
 
# Adjusted Graph for 2022-3 Fatalities
fatality_graph_2022_3 <- details_grouped_2022_3 %>%
  ggplot(aes(x = fatality_group, 
             y = avg_fatalities_by_type, 
             fill = fatality_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "2022-3: Fatalities by Group", subtitle = "Data from 2022-3", 
       x = "Weather Group",  
       y = "Average Fatalities") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Adjusted Graph for 2022-3 Damages
damage_graph_2022_3 <- details_grouped_2022_3 %>%
  ggplot(aes(x = damage_group, 
             y = avg_damage_by_type, 
             fill = damage_group)) +
  stat_summary(fun = "mean", geom = "bar", position = "dodge", alpha = 0.7) +
    geom_text(stat = "summary", fun = "mean", aes(label = round(after_stat(y), 2)), 
            vjust = -0.5, size = 3) +
  labs(title = "2022-3: Damages by Group", subtitle = "Data from 2022-3", 
       x = "Weather Group",  
       y = "Average Damages (USD)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5)  
  )
 
# Combine plots
combined_2022_3 <- fatality_graph_2022_3 + damage_graph_2022_3
 
# Display plots
combined_2022_3
```
Below are the tibbles showing event types, classifications, and impact averages for the entire dataset, 1996–7, and 2022–3. This allows us to view the changes and cla
``` {r chunk_6, echo=FALSE}
# Exact values for the original chart
details_grouped %>%
  select(event_type, fatality_group, damage_group, avg_fatalities_by_type, avg_damage_by_type) %>%
  arrange(fatality_group, damage_group)
 
# Exact values for 1996-7
details_grouped_1996_7 %>%
  select(event_type, fatality_group, damage_group, avg_fatalities_by_type, avg_damage_by_type) %>%
  arrange(fatality_group, damage_group) 
 
# Exact values for 2022-3
details_grouped_2022_3 %>%
  select(event_type, fatality_group, damage_group, avg_fatalities_by_type, avg_damage_by_type) %>%
  arrange(fatality_group, damage_group)
```
## Changes
All averaged values decreased from 1996–7 to 2022–3. 
First, by creating and joining the year groups, we produced a tibble showing the percent and absolute change for each event type. Next, we calculated percent and absolute changes for the global averages of each period. Finally, we generated a simple percent change for each more/less category. 
``` {r chunk_7}
# Create year groups
details_grouped_1996_7 <- details_grouped_1996_7 %>%
  mutate(year_group = "1996-1997")
 
details_grouped_2022_3 <- details_grouped_2022_3 %>%
  mutate(year_group = "2022-2023")
 
# Combine the data-sets
combined_data <- full_join(
  details_grouped_1996_7, 
  details_grouped_2022_3, 
  by = "event_type", 
  suffix = c("_1996_7", "_2022_3")
)
 
# Calculate differences and percentage changes
combined_data <- combined_data %>%
  mutate(
    change_fatalities = avg_fatalities_by_type_2022_3 - avg_fatalities_by_type_1996_7,
    change_damage = avg_damage_by_type_2022_3 - avg_damage_by_type_1996_7,
    percent_change_fatalities = (change_fatalities / avg_fatalities_by_type_1996_7) * 100,
    percent_change_damage = (change_damage / avg_damage_by_type_1996_7) * 100
  )
 
# View
combined_data %>%
  select(event_type, change_fatalities, percent_change_fatalities, change_damage, percent_change_damage)
 
# Compare the global averages between the two groups
comparison <- tibble(
  metric = c("Fatalities", "Damages"),
  avg_1996_7 = c(global_averages_1996_7$global_avg_fatalities, global_averages_1996_7$global_avg_damage),
  avg_2022_3 = c(global_averages_2022_3$global_avg_fatalities, global_averages_2022_3$global_avg_damage),
  absolute_change = avg_2022_3 - avg_1996_7,
  percent_change = (absolute_change / avg_1996_7) * 100
)
 
# View
comparison
 
# Final category changes calculations
Percent_change_less_fatalities <- ((.02-.03)/.03)*100
Percent_change_more_fatalities <- ((.55-1.07)/1.07)*100
Percent_change_less_damages <- ((46507.11-384199.67)/384199.67)*100
Percent_change_more_damages <- ((6543117.28-7033105.21)/7033105.21)*100 
 
# Laying it all out
Percent_change_less_fatalities
Percent_change_more_fatalities
Percent_change_less_damages
Percent_change_more_damages
```
 
# Findings
While total damages and fatalities rose, average impact per event declined. This suggests that increased storm frequency has affected people and property more than storm strength.  
However, a limitation of this dataset is the inability to fully assess storm severity without accounting for infrastructure strength, which was unavailable. 












## Event Proportions and ENSO Status

To simplify the investigation of the impact of El Niño Southern Oscillation (ENSO) on our data, irrelevant variables were removed from the `details_clean` data set (Chunk 1). Then, using data from NOAA and NASA, a column displaying the ENSO status of each year was added and the resulting data frame was saved as `details_enso` (Chunk 2). Finally, a table of expected and observed events grouped by ENSO status was produced for initial analysis (Chunk 3). `exp_prop` acts as a null hypothesis-- assuming ENSO status has no impact on event frequency.The resulting table shows that the observed frequency is relatively similar to expected, meaning we fail to reject the null hypothesis.

```{r chunk_1}
#remove irrelevant variables for ENSO analysis
details_smaller <- details %>% 
  select(-region_fips,
         -cz_type, 
         -cz_fips, 
         -cz_timezone, 
         -wfo, 
         -source, 
         -tor_other_cz_state, 
         -tor_other_cz_fips, 
        )
```

```{r chunk_2}
#add a column for ENSO status
details_enso <- details_smaller %>% 
  mutate(enso_status = case_when(
         year(begin_dt) %in% c(1998, 
                               2003, 
                               2007, 
                               2010, 
                               2016, # 1998-2022 -> (NOAA, 2022)
                               2023 #2023 > (NASA, 2024)
                               ) ~ 'el_nino',
         year(begin_dt) %in% c(1999, 
                               2000, 
                               2008, 
                               2011, 
                               2012, 
                               2021, 
                               2022
                               ) ~ 'la_nina',
         TRUE ~ 'neutral'
  ))
```

```{r chunk_3 / table_1}
#create frequency table of ENSO events
details_enso %>% 
  group_by(enso_status) %>% 
  summarise(obs_count = n(), #observed event count
            obs_prop = n() / nrow(details_enso), #observed event proportion
            exp_prop = length(unique(year(begin_dt))) #expected event proportion
            / length(unique(year(details_enso$begin_dt)))
  )
```

## Event Types and ENSO Status
Though event frequency does not appear significantly impacted by ENSO, we were also interested in if the proportion of event types varies by ENSO status. This was explored by calculating event type proportions across all years of a common ENSO status (Chunk 4). Event types with the largest difference in proportion between El Niño and La Niña years were visualised (Chunk 5).

```{r chunk_4}
#calculate proportion in El Niño years
el_nino_prop <- details_enso %>% 
  filter(enso_status == 'el_nino') %>%
  count(event_type) %>% 
  mutate(proportion_en = n / sum(n))

#calculate proportion in La Niña years
la_nina_prop <- details_enso %>% 
  filter(enso_status == 'la_nina') %>%
  count(event_type) %>% 
  mutate(proportion_ln = n / sum(n))

#calculate proportion in neutral years
neutral_prop <- details_enso %>% 
  filter(enso_status == 'neutral') %>%
  count(event_type) %>% 
  mutate(proportion_n = n / sum(n))

#join proportion tibbles
proportions_enso <- full_join(el_nino_prop, neutral_prop, by = 'event_type') %>% 
  full_join(la_nina_prop, by = 'event_type') %>% 
  top_n((abs(proportion_en - proportion_ln)), n = 10) %>% 
  pivot_longer(cols = c(proportion_en, proportion_ln, proportion_n), 
               names_to = 'enso_status', 
               values_to = 'proportion'
               )
```

```{r chunk_5}
#reorder ENSO status for plotting
proportions_enso <- proportions_enso %>%
  mutate(enso_status = factor(enso_status, levels = c("proportion_en", "proportion_n", "proportion_ln")))

#create visualisation
proportions_enso %>% 
  ggplot(aes(fill = enso_status, 
             x = proportion, 
             y = reorder(event_type, proportion)) #make plot more intuitive
         ) +
  
  geom_col(position = 'dodge') + #separate columns -> easier to compare
  
  labs(
    x = "Proportion of Total Events in Common ENSO Status Years",
    y = "Event Type",
    title = "Event Proportion by Event Type and ENSO Status",
    subtitle = "Event types with largest difference between El Niño and La Niña years",
    fill = 'ENSO Status'
  ) +
  
  scale_fill_manual(values = c('proportion_en' = 'midnightblue', 
                               'proportion_n' = 'purple2', 
                               'proportion_ln' = 'mediumorchid2'),
    labels = c('El Niño', 'Neutral', 'La Niña')
  )
```

## Change in ENSO Statuses Over Time

As a part of initial analysis, we noted a change in the slopes of event count grouped by ENSO status (Chunk 6). Events in La Niña years appear to be  increasing in monthly frequency at a slightly greater rate than those of a different ENSO status (Chunk 7).

```{r chunk_6}
slope_exploration <- details_enso %>%
  #group by month to increase number of observations, avoid daily outliers
  group_by(floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))
  
#create visualisation (including individual observations)
slope_exploration %>% 
  ggplot(mapping = aes(x = floor_date, y = log(n), color = enso_status)) +
  
  geom_point() +
  
  geom_smooth(method = lm, se = FALSE) + #add linear model
  
  scale_color_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
    labels = c('El Niño', 'Neutral', 'La Niña')
  ) +
  
  labs(
    x = "Date",
    y = "Number of Events per Month (log-scale)",
    title = "Event Count Over Time, Grouped by ENSO Status",
    subtitle = "Includes Monthly Observations and Linear Model",
    color = "ENSO Status"
  )
```

```{r chunk_7}
#plot only linear models, excluding individual observations
slope_exploration %>% 
  ggplot(mapping = aes(x=floor_date, y = log(n), color = enso_status)) +
  
  geom_smooth(method = lm, se= FALSE) +
  
  scale_color_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
    labels = c('El Niño', 'Neutral', 'La Niña')) +
  
  labs(
    x = "Date",
    y = "Number of Events per Month (log scale)",
    title = "Event Count Over Time, Grouped by ENSO Status",
    subtitle = "Includes Only Linear Model",
    color = "ENSO Status"
  )
  
```

To test if this difference in slope is statistically significant, each linear model was bootstrapped (Chunk 8). The results of the bootstrapping procedure are displayed in the output of Chunk 9. No significant difference is apparent due to the overlap in confidence intervals, though this could be due to sample size.

```{r chunk_8}
#calculate number of events per month for each ENSO status
#store in seperate tibbles
by_month_en <- details_enso %>% 
  filter(enso_status == 'el_nino') %>% 
  group_by(en_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

by_month_ln <- details_enso %>%
  filter(enso_status == 'la_nina') %>% 
  group_by(ln_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

by_month_n <- details_enso %>%
  filter(enso_status == 'neutral') %>% 
  group_by(n_floor_date = floor_date(begin_dt, 'month')) %>% 
  summarise(n = n(), enso_status = unique(enso_status))

#create bootstrapping function
bootstrapping <- function(df, floor_name){
  # set a seed
  set.seed(30305)
  floor_name <- sym(floor_name)
  
  # take 1000 bootstrap samples
  enso_boot <- bootstraps(df, times = 1000)
  
  # for each sample
  # fit a model and save output in model column
  # tidy model output and save in coef_info column 
  enso_slope_models <- enso_boot %>%
    mutate(
      model = map(splits, ~ lm(n ~ !!floor_name, data = .)),
      coef_info = map(model, tidy)
    )
    
  # un-nest coef_info (for intercept and slope)
  enso_coefs <- enso_slope_models %>%
    unnest(coef_info)
    
  # calculate 95% confidence interval
  int_pctl(enso_slope_models, coef_info)
}

#display bootstrapped slopes in a tibble
el_nino_slope <- bootstrapping(by_month_en, 'en_floor_date')
View(el_nino_slope)
la_nina_slope <- bootstrapping(by_month_ln, 'ln_floor_date')
View(la_nina_slope)
neutral_slope <- bootstrapping(by_month_n, 'n_floor_date')
View(neutral_slope)

```

```{r chunk_9}
#assign ENSO status to bootstrapped results before binding
el_nino_slope$enso_status <- "el_nino"
la_nina_slope$enso_status <- "la_nina"
neutral_slope$enso_status <- "neutral"

combined_slopes <- dplyr::bind_rows(el_nino_slope, neutral_slope, la_nina_slope)

#remove intercept error bars from bound tibble
combined_slopes %>% 
  filter(term != '(Intercept)') %>% 
  
  #create visualisation 
  ggplot(mapping = aes(x = enso_status, y = .estimate, fill = enso_status)) +
  
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) + 
  
  #assign CI values to error bar
  geom_errorbar(aes(ymin = .lower, ymax = .upper), 
                position = position_dodge(width = 0.8),
                width = 0.25
                ) +
  
  scale_fill_manual(values = c('el_nino' = 'midnightblue',
                                'neutral' = 'purple2', 
                                'la_nina' = 'mediumorchid2'
                                ),
  labels = c('El Niño', 'La Niña', 'Neutral')
  ) +
  
  #correct axis labelling
  scale_x_discrete(
    labels = c('el_nino' = 'El Niño', 
               'la_nina' = 'La Niña', 
               'neutral' = 'Neutral'
               )
    ) +
  
  labs(
    x = "ENSO Status",
    y = "True Slope (of log-scaled model)",
    title = "Bootstrapped Linear Model Slopes by ENSO Status",
  ) +
  
  theme(legend.position = 'none')
```

# Citations 

Ocean Surface Topography from Space. (2024). *El Niño 2023 \| El Niño/La Niña Watch & PDO*. [online] Available at: <https://sealevel.jpl.nasa.gov/data/el-nino-la-nina-watch-and-pdo/el-nino-2023/.>

FORMAT THIS: <https://psl.noaa.gov/enso/past_events.html>













When we analysed fatalities in the data, we wanted to look at underrepresented age groups in fatalities. We chose those between 0 and 13 years old and those who are 70 and over to be our groups of interest. We separated our weather events into weather categories to interpret the types of weather that affect each age group the most.

```{r seperate weather events, echo=FALSE}

events<- details %>% select(event_id, event_type)

# Seperating events into categories
# Marine - water related events or events relating to bodies of water
# snow/ice - events related to cold or winter weather
# Atmospheric - electric storms and other atmospheric events
# Tropical - tropical weather events, hurricanes etc
# heat/temp - events relating towards high temperatures or fire
# rain - rain and hail

events <-events %>% 
  mutate(weather_cat = case_when(
    event_type == "Astronomical Low Tide" ~ "Marine",
    event_type == "Coastal Flood" ~ "Marine",
    event_type == "Debris Flow" ~ "Marine",
    event_type == "Flash Flood" ~ "Marine",
    event_type == "Flood" ~ "Marine",
    event_type == "High Surf" ~ "Marine",
    event_type == "Lake-Effect Snow" ~ "Marine",
    event_type == "Lakeshore Flood" ~ "Marine",
    event_type == "Marine Hail" ~ "Marine",
    event_type == "Marine High Wind" ~ "Marine",
    event_type == "Marine Strong Wind" ~ "Marine",
    event_type == "Marine Thunderstorm Wind" ~ "Marine",
    event_type == "Rip Current" ~ "Marine",
    event_type == "Seiche" ~ "Marine",
    event_type == "Storm Surge/Tide" ~ "Marine",
    event_type == "Tsunami" ~ "Marine",
    event_type == "Waterspout" ~ "Marine",
    event_type == "Sneakerwave" ~ "Marine",
    event_type == "Marine Lightning" ~ "Marine",
    event_type == "Marine Tropical Depression" ~ "Marine",
    event_type == "Marine Hurricane/Typhoon" ~ "Marine",
    event_type == "Marine Dense Fog" ~ "Marine",
    event_type == "Avalanche" ~ "Ice/Snow",
    event_type == "Blizzard" ~ "Ice/Snow",
    event_type == "Cold/Wind Chill" ~ "Ice/Snow",
    event_type == "Extreme Cold/Wind Chill" ~ "Ice/Snow",
    event_type == "Frost/Freeze" ~ "Ice/Snow",
    event_type == "Ice Storm" ~ "Ice/Snow",
    event_type == "Heavy Snow" ~ "Ice/Snow",
    event_type == "Sleet" ~ "Ice/Snow",
    event_type == "Winter Storm" ~ "Ice/Snow",
    event_type == "Winter Weather" ~ "Ice/Snow",
    event_type == "Dense Fog" ~ "Atmospheric",
    event_type == "Dense Smoke" ~ "Atmospheric",
    event_type == "Freezing Fog" ~ "Atmospheric",
    event_type == "Funnel Cloud" ~ "Atmospheric",
    event_type == "High Wind" ~ "Atmospheric",
    event_type == "Lightning" ~ "Atmospheric",
    event_type == "Strong Wind" ~ "Atmospheric",
    event_type == "Northern Lights" ~ "Atmospheric",
    event_type == "Thunderstorm Wind" ~ "Atmospheric",
    event_type == "Tornado" ~ "Atmospheric",
    event_type == "Dust Devil" ~ "Atmospheric",
    event_type == "Dust Storm" ~ "Atmospheric",
    event_type == "Hurricane (Typhoon)" ~ "Tropical",
    event_type == "Hurricane" ~ "Tropical",
    event_type == "Tropical Depression" ~ "Tropical",
    event_type == "Tropical Storm" ~ "Tropical",
    event_type == "Heat" ~ "Fire/HiTemps",
    event_type == "Excessive Heat" ~ "Fire/HiTemps",
    event_type == "Volcanic Ash" ~ "Fire/HiTemps",
    event_type == "Volcanic Ashfall" ~ "Fire/HiTemps",
    event_type == "Drought" ~ "Fire/HiTemps",
    event_type == "Wildfire" ~ "Fire/HiTemps",
    TRUE ~ "Rain"
  ))


```

```{r age groups}

median_fatality <- filter(fatalities, fatality_age < 70 & fatality_age > 13)%>%
  right_join(events, median_fatality)

elderly_fatality <- filter(fatalities, fatality_age >= 70) %>%
  right_join(events, elderly_fatality)

elderly_fatality <- filter(fatalities, fatality_age >= 70) %>% 
  elderly_fatality <- right_join(events, elderly_fatality)

```

```{r, echo = FALSE}

# plot of age groups and fatality type

ggplot(na.omit(combine_fatality), aes(x=age_group, fill=weather_cat)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2") +
  labs(x= "Age group", y= "Proportion of fatalities for each category", fill= "Weather Categories", title = "Proportion of Each Age Group Affected by Each Weather Category") +
  theme_bw() # Proportion of each age group affected by different weather categories labelled above

```

We see how each of the age groups is disproportionately affected by each weather category. It's important to understand this data as it is very useful to know who is most vulnerable to weather events. From this analysis we see that the Fire/HiTemps and Tropical category is most popular with the elderly age group and Atmospheric and Marine is the most popular with the young age group.


```{r, echo=FALSE}

# Elderly - where is it affected?

# most affected by fire hi temps and tropical unlike the median category

elderly_category <- filter(elderly_fatality, weather_cat == "Fire/HiTemps" | weather_cat == "Tropical") %>%
ggplot(aes(x=event_type, fill=weather_cat)) +
  geom_bar() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x ="Weather Event", y ="Amount of Fatalities", title="Elderly Fatalities by Most Popular Weather Categories", fill="Weather Category") +
  theme_bw()+
  coord_flip() 

# Young - Where is it affected?

young_category <- filter(young_fatality, weather_cat == "Marine" | weather_cat == "Atmospheric") %>%
ggplot(aes(x=event_type, fill=weather_cat)) +
  geom_bar() +
  scale_fill_brewer(palette = "Dark2") +
  labs(x ="Weather Event", y ="Amount of Fatalities", title="Young Fatalities by Most Popular Weather Categories", fill="Weather Category") +
  theme_bw()+
  coord_flip() # young fatality age group plotted with the 2 largest disproportionate causes of fatalities

young_category
elderly_category

```

From the graphs, we find that the young age group is most affected by floods and tornadoes, whilst the elderly group is most affected by heat and hurricanes.

```{r, echo = FALSE }

# How has weather fatalities changed over time?

combine_facet <- ggplot(na.omit(combine_fatality), aes(x=age_group, fill=weather_cat)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2") +
  labs(x= "Age group", y= "Proportion of fatalities for each category over time", fill= "Weather Categories", title = "Proportion of Each Age Group Affected by Each Weather Category") +
  scale_x_discrete(labels = c(
    "elderly" = "E",
    "median" = "M",
    "young" = "Y"
  )) +
  theme_bw() +
  theme(axis.text.y = element_blank()) +
  facet_wrap(year(ymd)~.) 

# proportionality of weather categories and fatality over years

combine_facet

```

The graph shows how this proportion has changed over the years, and we can even see in years of extraordinary weather (such as 2005 with hurricane Katrina & 2011 with major tornado outbreaks), weather events still have a large effect on different age groups.  

```{r, echo=FALSE}

# are Fire/HiTemps increasing?

combine_lines <- filter(combine_fatality, weather_cat == "Fire/HiTemps") %>%
  ggplot(aes(x=year(ymd), colour=event_type)) +
  geom_line(aes(fill=..count..),stat="bin", binwidth = 1, linewidth = 1) +
  labs(x="Years",y="Amount of Fatalities",title="Amount of Fire and High Temperature Fatalities Over Time",colour="Event Types") +
  scale_colour_manual(values=c("midnightblue", "purple2", "mediumorchid2")) +
  theme_bw() 

# plot looks at the changes of Fire/HiTemps weather_cat over years

combine_lines

```

We also found a slight trend where in the past 20 years the fatalities in the Fire/HiTemps category have been increasing. This could possibly be explained by climate change having an effect on the weather around the US.


## References

NOAA Weather service, provided data for the project, accessed at URL: https://www.ncdc.noaa.gov/stormevents/faq.jsp
NOAA Weather service, data format guide, accessed at URL: https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf
























The write-up of your project and findings go here.

Think of this as the text of your presentation with some extra detail to cover what there was not time to discuss in the presentation.
This might include any assumptions you made when doing your analysis, any limitations of the work you have done or any ideas you have for future work.
Feel free to split this section into subsections to make it easier to read.

The length should be roughly 1,500 words.
If you want to use a word count addin, you can install this by copying and pasting the following into RStudio:

`devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)`

You will then need to restart RStudio.
Once you have done that, select the text you want to count the words of, go to Addins, and select the `Word count` addin.
This addin counts words using two different algorithms, but the results should be similar and as long as you're in the ballpark of 1,500 words, you're good!
The addin will ignore code chunks and only count the words in prose.
If you don't want to use the addin you can always copy and paste the text into Microsoft Word to do a Word Count!

You can also load your data here and present any analysis results / plots.
Make sure to hide your code with `echo = FALSE` unless the point you are trying to make is about the code itself, in which case you should show your code.

## Impact of Storm Events

We also chose to explore the impact of weather events across the United States. Initially, unessesary variables were removed from `details_clean.rds`.Then, the data was filtered to find the most severe weather events using `damage_total`, and stored in either `severe_details` and `not_severe_details`, with the latter containing all other weather events. This was then joined to `fatalities_clean.rds`.


Considering the total number of weather events, we found Texas experienced significantly more weather events than any other state.This could potentially be explained by its considerable size.


## References

NOAA Weather service, provided data for the project, accessed at URL: <https://www.ncdc.noaa.gov/stormevents/faq.jsp> NOAA Weather service, data format guide, accessed at URL: <https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf>
